# DocStruct

**Document Classification & Extraction Pipeline**

Transform unstructured business documents into structured, searchable JSON data.

## Features

- **ML Classification** â€” TF-IDF + Logistic Regression classifies documents into 4 types
- **LLM Extraction** â€” GPT-4o-mini extracts structured fields from raw text
- **Export** â€” Download results as CSV or JSON
- **Streamlit UI** â€” Interactive web interface

## Document Types

| Type | Example Fields |
|------|---------------|
| ðŸ“Š Invoice | order_id, customer, line_items, total |
| ðŸ“¦ Shipping Order | tracking_number, carrier, destination, recipient |
| ðŸ›’ Purchase Order | po_number, vendor, items, total_value |
| ðŸ“‹ Inventory Report | warehouse, items, quantities |

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Set OpenAI API key
echo "OPENAI_API_KEY=sk-your-key" > .env

# Run the app
streamlit run app.py
```

Open http://localhost:8501

## How It Works

```
Raw Document â†’ [ML Classifier] â†’ Document Type â†’ [LLM Extractor] â†’ Structured JSON
```

1. **Load** sample documents from HuggingFace dataset
2. **Classify** using TF-IDF vectorization + Logistic Regression (87% avg confidence)
3. **Extract** structured data using GPT-4o-mini with Pydantic schemas
4. **Export** to CSV/JSON for downstream use

## Project Structure

```
â”œâ”€â”€ app.py              # Streamlit UI
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ classifier.py   # TF-IDF + Logistic Regression
â”‚   â”œâ”€â”€ extractor.py    # OpenAI LLM extraction
â”‚   â”œâ”€â”€ pipeline.py     # Orchestration
â”‚   â””â”€â”€ schemas.py      # Pydantic models
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ models/             # Trained classifier
â””â”€â”€ requirements.txt
```

## Tech Stack

- **Classification**: scikit-learn (TF-IDF, LogisticRegression)
- **Extraction**: OpenAI GPT-4o-mini
- **Validation**: Pydantic
- **UI**: Streamlit
- **Data**: HuggingFace datasets

## License

MIT
